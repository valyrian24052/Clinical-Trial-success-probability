{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Hackathon Starter:Running Ollama + LangChain in Google Colab\n",
        "\n",
        "Welcome, canddiates! This notebook shows you how to set up and run a powerful local LLM using **Ollama** and then interact with it using the **LangChain** framework.\n",
        "> *   Please note that you are **not required** to use Generative AI, if your solution is better served by traditional NLP models (NER, text classification, etc.) or other techniques, you are completely free to pursue that path.\n",
        "\n",
        "For anyone who wishes to use Gen AI and dont have access to commercial APIs from Open AI or Gemini or Claude, this setup creates a local API server for the model, which is a great pattern for building real-world applications.\n",
        "\n",
        "**Our Model of Choice:** **Microsoft's Phi-4-Mini**. It's a highly capable 3.8B parameter model that is very fast and fits perfectly on a Colab T4 GPU.\n",
        "\n",
        "### ✅ Step 0: Pre-flight Check\n",
        "\n",
        "1.  **Enable GPU:** Ensure your runtime is using a GPU. Go to `Runtime` -> `Change runtime type` and select `T4 GPU`.\n",
        "2.  **Check GPU Status:** Run the cell below to confirm. You should see details about the \"Tesla T4\" GPU."
      ],
      "metadata": {
        "id": "RiZDSHvD3Q3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU Status\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9JtKbFTt4TEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 1: Install Necessary Libraries\n",
        "\n",
        "First, we need to install the Python packages for this project:\n",
        "- `langchain` & `langchain-community`: The core libraries for building applications with LLMs.\n",
        "- `colab_xterm`: This package lets us open a terminal right here in Colab, which we need to install and run Ollama."
      ],
      "metadata": {
        "id": "1OgiRO594qVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python libraries\n",
        "!pip install -q -U langchain langchain-community langchain-ollama colab_xterm\n"
      ],
      "metadata": {
        "id": "1Pa2S8XT4rjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 2: Set Up and Run Ollama\n",
        "\n",
        "Ollama is a tool that lets us easily run open-source LLMs locally. We will install it and start the Ollama server in the background.\n",
        "\n",
        "**This is a two-part step:**\n",
        "\n",
        "1.  **Launch the Terminal:** Run the cell below to open a terminal window.\n",
        "2.  **Follow the instructions** that appear under the terminal cell."
      ],
      "metadata": {
        "id": "1ttDFreI4vhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch the terminal\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "RFSEJMGB6aru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ❗️INSTRUCTIONS FOR THE TERMINAL (Run these commands inside the black terminal window above)\n",
        "\n",
        "You now have a command-line terminal. You need to run three commands inside it, one after the other.\n",
        "\n",
        "**Command 1: Install Ollama**\n",
        "\n",
        "*   Copy and paste this command into the terminal and press Enter. It will download and run the Ollama installation script\n",
        "*   \"curl -fsSL https://ollama.com/install.sh | sh\"\n",
        "\n",
        "**Command 2: Start the Ollama Server in the Background**\n",
        "\n",
        "\n",
        "*   Below command starts the Ollama server. We use `nohup` and `&` to make sure it keeps running in the background even if you close the terminal.\n",
        "*   \"nohup ollama serve &\"\n",
        "*    You might see a message like `[1] 12345`. This is normal. Press Enter again to get back to the prompt. The server is now running.\n",
        "\n",
        "**Command 3: Pull the Phi-4 Mini Model from repo**\n",
        "\n",
        "\n",
        "1.   \"ollama serve & ollama pull phi4-mini\"\n",
        "2.   once you've done this, you can move on to the next step\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "blDOqFHa8DWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Step 3: Connect and Query with LangChain\n",
        "\n",
        "Now that our Ollama server is running and has the `phi4-mini` model, we can connect to it from our Python code using LangChain.\n",
        "\n",
        "The `langchain_ollama` class makes this incredibly simple."
      ],
      "metadata": {
        "id": "KrqG1TQ9_OF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instantiate and Query the Model\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Instantiate the Ollama model\n",
        "# The 'model' parameter MUST match the name you used in 'ollama pull'\n",
        "try:\n",
        "    llm = OllamaLLM(model=\"phi4-mini\")\n",
        "    print(\"✅ Ollama model instantiated successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error instantiating model: {e}\")\n",
        "    print(\"   Please make sure you have run the Ollama setup steps in the terminal above.\")\n",
        "\n",
        "# Now, let's ask it a question!\n",
        "prompt = \"Why is the sky blue? Explain it like I'm a 10 year old.\"\n",
        "\n",
        "print(\"\\n--- Sending prompt to model ---\")\n",
        "print(f\"PROMPT: {prompt}\")\n",
        "\n",
        "# Use the .invoke() method to get a response\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(\"\\n--- Model Response ---\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "-612mAxW8z9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💡 Important Note: Your Path Forward\n",
        "\n",
        "> *   You now have a fully functional **local LLM**! You can use this as the core of your solution, building simple or complex Gen AI pipelines as needed.\n",
        ">\n",
        "> *   Please remember that this local setup is provided as a **powerful fallback option**. It ensures everyone can participate, especially those without access to paid commercial APIs like OpenAI's `ChatGPT` or Google's `Gemini`.\n",
        ">\n"
      ],
      "metadata": {
        "id": "v7O6fJcGD4zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 💡 Troubleshooting & Next Steps\n",
        "\n",
        "*   **\"Connection refused\" error?** The Ollama server probably isn't running. Go back to the terminal in Step 2 and run `nohup ollama serve &` again.\n",
        "*   **\"Model not found\" error?** Make sure the model name in `OllamaLLM(model=\"...\")` exactly matches the name you used in `ollama pull ...`. You can check available models with `ollama list` in the terminal.\n",
        "*   **Colab session disconnected?** If your Colab runtime resets, you will have to re-run the steps, including the commands in the terminal to start and pull the model again.\n",
        "*   **Want to try a different model?** Go for it! You can pull another model like `llama3:8b` or `mistral` in the terminal (`ollama pull llama3:8b`) and then change the model name in your Python code (`llm = OllamaLLM(model=\"llama3:8b\")`)."
      ],
      "metadata": {
        "id": "jIGGSxRJ84zj"
      }
    }
  ]
}